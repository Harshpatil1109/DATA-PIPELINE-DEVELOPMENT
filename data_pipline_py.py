# -*- coding: utf-8 -*-
"""data_pipline.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DwyuUMdsXjqGK66m6m5dbIgH5YbgA_ys
"""

sklearn.preprocessing import StandardScalerimport pandas as pd
from

def extract_data(file_path):
    """
    Extract data from CSV file.
    """
    print("Extracting data...")
    data = pd.read_csv(file_path)
    print(f"Data extracted: {data.shape[0]} rows, {data.shape[1]} columns.")
    return data

def transform_data(df):
    """
    Transform the data by handling missing values and scaling numeric columns.
    """
    print("Transforming data...")

    # Fill missing values
    df = df.fillna(df.mean(numeric_only=True))

    # Scale numeric features
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    scaler = StandardScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    print("Data transformation complete.")
    return df

def load_data(df, output_path):
    """
    Load the transformed data to a new CSV file.
    """
    print("Loading data...")
    df.to_csv(output_path, index=False)
    print(f"Data successfully saved to {output_path}")

def run_pipeline(input_file, output_file):
    print("=== Starting ETL Pipeline ===")
    data = extract_data(input_file)
    transformed_data = transform_data(data)
    load_data(transformed_data, output_file)
    print("=== Pipeline Completed Successfully ===")

if __name__ == "__main__":
    input_path = "sample_data.csv"
    output_path = "processed_data.csv"
    run_pipeline(input_path, output_path)